# ğŸ”’ AnÃ¡lisis de Clustering Forense - Ciberataques

Proyecto de anÃ¡lisis forense para determinar si los ciberataques registrados provienen de 2 o 3 atacantes distintos utilizando tÃ©cnicas de Machine Learning y clustering.

## ğŸ“‹ DescripciÃ³n del Proyecto

Este proyecto utiliza PySpark MLlib para realizar un anÃ¡lisis exhaustivo de clustering sobre datos de ciberataques. El objetivo principal es identificar patrones de comportamiento que permitan determinar cuÃ¡ntos atacantes distintos estÃ¡n detrÃ¡s de los incidentes registrados.

### CaracterÃ­sticas Principales

- **AnÃ¡lisis Exploratorio de Datos (EDA)** completo con visualizaciones
- **Clustering con K-Means y GMM** (Gaussian Mixture Models)
- **ValidaciÃ³n con mÃºltiples mÃ©tricas**: Silhouette Score, ARI, BIC
- **IngenierÃ­a de caracterÃ­sticas** (ratios por minuto)
- **ReducciÃ³n dimensional con PCA** para visualizaciÃ³n
- **Dashboard interactivo** con Streamlit
- **VisualizaciÃ³n geogrÃ¡fica** de ataques

## ğŸ› ï¸ Requisitos

### Entorno de Desarrollo

- **Python**: 3.13.5
- **Gestor de paquetes**: Conda (recomendado)

### Dependencias Principales

```
- pyspark 4.0.1
- pandas
- numpy
- matplotlib
- seaborn
- plotly
- scikit-learn
- streamlit
- reportlab
```

## ğŸ“¦ InstalaciÃ³n

### 1. Clonar el Repositorio

```powershell
git clone https://github.com/Rodrimansidub14/ConsultingHacking.git
cd ConsultingHacking
```

### 2. Crear el Entorno Conda

```powershell
conda env create -f environment.yml
```

O crear manualmente:

```powershell
conda create -n py-3.13.5 python=3.13.5
conda activate py-3.13.5
```

### 3. Instalar Dependencias

Si creaste el entorno manualmente:

```powershell
conda install numpy pandas matplotlib seaborn reportlab
pip install pyspark plotly streamlit scikit-learn
```

## ğŸš€ EjecuciÃ³n del Proyecto

### AnÃ¡lisis en Jupyter Notebook

1. **Activar el entorno**:
   ```powershell
   conda activate py-3.13.5
   ```

2. **Abrir el notebook**:
   ```powershell
   jupyter notebook ConsultingNB.ipynb
   ```
   O simplemente abrir el archivo en VS Code.

3. **Ejecutar las celdas en orden** para:
   - Cargar y explorar los datos
   - Realizar feature engineering
   - Entrenar modelos de clustering
   - Generar visualizaciones
   - Exportar resultados a `data/hack_data_clustered.csv`

### Dashboard Interactivo

1. **Asegurarse de que el archivo de datos procesados existe**:
   - El notebook debe haber exportado `data/hack_data_clustered.csv`

2. **Activar el entorno** (si no estÃ¡ activo):
   ```powershell
   conda activate py-3.13.5
   ```

3. **Ejecutar el dashboard**:
   ```powershell
   streamlit run dashboard.py
   ```

4. **Acceder al dashboard**:
   - Abre tu navegador en: `http://localhost:8501`

### PÃ¡ginas del Dashboard

El dashboard incluye 5 secciones principales:

1. **ğŸ“Š Resumen Ejecutivo**
   - Objetivo y conclusiones del anÃ¡lisis
   - KPIs principales (Silhouette Score, ARI, Balance)
   - VisualizaciÃ³n PCA de separaciÃ³n de clusters

2. **ğŸ“ˆ AnÃ¡lisis Detallado**
   - Boxplots interactivos por mÃ©trica
   - GrÃ¡ficos de dispersiÃ³n con lÃ­neas de tendencia
   - ComparaciÃ³n de comportamiento entre clusters

3. **ğŸ‘¥ Perfiles de ClÃºsteres**
   - EstadÃ­sticas descriptivas por cluster
   - Histogramas y violin plots
   - ComparaciÃ³n de distribuciones

4. **ğŸŒ DistribuciÃ³n GeogrÃ¡fica**
   - Mapa interactivo de ataques por ubicaciÃ³n
   - TamaÃ±o de burbujas segÃºn frecuencia de ataques
   - Top 10 ubicaciones por cluster
   - MÃ©tricas geogrÃ¡ficas

5. **ğŸ”§ Detalles TÃ©cnicos**
   - Vista de datos procesados
   - Descarga de resultados en CSV
   - MetodologÃ­a completa

## ğŸ“Š Estructura del Proyecto

```
ConsultingHacking/
â”‚
â”œâ”€â”€ ConsultingNB.ipynb          # Notebook principal con anÃ¡lisis completo
â”œâ”€â”€ dashboard.py                # AplicaciÃ³n Streamlit
â”œâ”€â”€ environment.yml             # EspecificaciÃ³n del entorno conda
â”œâ”€â”€ README.md                   # Este archivo
â”‚
â””â”€â”€ data/
    â”œâ”€â”€ hack_data.csv           # Datos originales
    â””â”€â”€ hack_data_clustered.csv # Datos procesados con clusters (generado)
```
